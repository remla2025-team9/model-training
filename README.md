# Restaurant Sentiment Analysis - Model Training

This repository contains scripts to train, evaluate, and export a sentiment analysis model for restaurant reviews. The project follows a structured layout inspired by Cookiecutter Data Science.

<a target="_blank" href="https://cookiecutter-data-science.drivendata.org/">
    <img src="https://img.shields.io/badge/CCDS-Project%20template-328F97?logo=cookiecutter" alt="Cookiecutter Data Science Template Badge" />
</a>

---

## Project Organization

The project follows a structure adapted from the Cookiecutter Data Science template:

```
├── .github/             # GitHub Actions workflows (CI/CD - Planned)
│   └── workflows/
├── data/
│   ├── raw/             # Original, immutable data (e.g., training_data.tsv)
│   └── processed/       # Processed data ready for modeling (features & labels)
├── docs/                # Project documentation (e.g., MkDocs)
├── models/              # Trained model artifacts (e.g., sentiment_classifier-nb-v1.0.0.joblib)
├── notebooks/           # Jupyter notebooks for exploration
├── reports/             # Evaluation metrics, figures
│   └── evaluation_metrics.json
├── src/       # Python source code for the pipeline
│   ├── __init__.py
│   ├── config.py        # Configuration variables (paths, defaults)
│   ├── dataset.py       # Data loading, preprocessing (feature generation), splitting
│   └── modeling/
│       ├── __init__.py
│       ├── train.py     # Model training script
│       └── predict.py   # Model evaluation script
├── .gitignore           # Files for Git to ignore
├── pyproject.toml       # Project metadata and tool configuration
├── requirements.txt     # Python dependencies
└── README.md            # This file
```

---

## Prerequisites

*   Python 3.8 or newer
*   Git

**Installation:**

1.  **Install Python dependencies:**
    It's highly recommended to use a virtual environment:
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    pip install -r requirements.txt
    ```
    The `requirements.txt` should include your custom preprocessing library:
    ```bash
    # In requirements.txt
    # ... other packages ...
    git+https://github.com/remla2025-team9/lib-ml.git # Or your specific preprocessing lib
    ```

---

## Option 1: Usage - Running the Pipeline with DVC
The pipeline is designed to be run using DVC (Data Version Control) for managing data and model artifacts. This allows for reproducibility and versioning of datasets and models.

### Prerequisites for DVC
*   Install AWS CLI and configure your credentials for the S3 remote storage.
    ```bash
    pip install awscli
    aws configure # Set your AWS Access Key, Secret Key, and default region
    ```
*   Install DVC by setting up the python virtual environment as described above

*   Initialize DVC in the project directory:
    ```bash
    dvc pull
    ```

*   Run the pipeline using DVC:
    ```bash
    dvc repro
    ```
### DVC Pipeline Steps

**Fetching the Data and Model Files:**

Running `dvc pull` will download the necessary data and model files from the S3 remote storage configured in `.dvc/config`. 

**Running the DVC Pipeline:**
The DVC pipeline consists of multiple stages defined in `dvc.yaml`. Executing `dvc repro` will run the pipeline stages in the following order:
1. **`perpare`**: processes raw data into structured and labeled datasets, ready for feature extraction and modeling
   1. Loads raw text data from a CSV/TSV file
   2. Preprocesses the text column which generates feature vectors and saves vectorizer models and preprocessed data
   3. Splits the dataset into training and test sets
   4. Saves the resulting feature matrices and label arrays for both train and test splits to disk for use in subsequent stages
2. **`featurize`**: preprocesses raw training data to generate and save a text vectorizer model, enabling reproducible feature extraction as part of the DVC pipeline
    1. Relies on the raw training data prepared in the previous `prepare` stage
    2. Reads the cleaned and organized text data to ensure consistent input for feature extraction
    3. Applies text preprocessing and vectorization to convert raw text reviews into numerical features
    4. Saves the resulting vectorizer model for use in downstream stages like training and inference
3. **`train`**:Takes the processed training features and labels produced by earlier pipeline stages and fits a machine learning model 
    1. Loads the preprocessed training features and labels generated by the `prepare` stage
    2. Initializes the chosen classifier (Logistic Regression or Gaussian Naive Bayes)
    3. Trains the classifier on the training data
    4. Saves the trained model to disk for later use in predictions and evaluations
4. **`evaluate`**: takes the trained model and the processed test data from previous pipeline stages, uses the model to make predictions, and computes evaluation metrics to assess model performance.
    1. Loads the processed test features and labels generated by the `prepare` stage.
    2. Loads the trained model produced by the `train` stage
    3. Uses the model to predict labels for the test data
    4. Calculates various evaluation metrics
    5. Saves all computed metrics to a JSON file for reporting and tracking model performance
## Option 2: Usage - Running the Pipeline Manually

**Step 1: Data Processing (Generate features & split data)**

This script loads the raw data, uses an external library for text preprocessing to generate features, splits the data into training and test sets, and saves the processed features and labels.

```bash
python -m src.dataset
```
*   **Inputs:** Reads from `data/raw/training_data.tsv` (or as specified by `--raw_data_path` argument or `src/config.py`).
*   **Outputs:**
    *   `data/processed/train_features.npz`
    *   `data/processed/train_labels.csv`
    *   `data/processed/test_features.npz`
    *   `data/processed/test_labels.csv`

**Step 2: Model Training**

This script loads the processed training features and labels, trains a classifier, and saves the trained model.

```bash
python -m src.modeling.train --model_type logistic
```
*   **Inputs:** Reads `data/processed/train_features.npz` and `data/processed/train_labels.csv`.
*   **Parameters:**
    *   `--model_type`: Choose 'nb' (Gaussian Naive Bayes) or 'logistic' (Logistic Regression).
*   **Outputs:** Saves the trained model to `models/model.joblib`

**Step 3: Model Evaluation**

This script loads the processed test features and labels, loads a trained model, makes predictions, and saves evaluation metrics.

```bash
python -m src.modeling.predict --model_path models/model.joblib
```
*   **Inputs:**
    *   Reads `data/processed/test_features.npz` and `data/processed/test_labels.csv`.
    *   `--model_path`: Path to the trained model file saved in Step 2.
*   **Outputs:**
    *   Prints classification report and confusion matrix to the console.
    *   Saves detailed metrics to `reports/evaluation_metrics.json`.

---

## Parameters

Key parameters for the scripts (e.g., test split ratio, model type) can be passed as command-line arguments as shown above. Default file paths are managed in `src/config.py`.
*(A `params.yaml` file is included for planned DVC integration, where these parameters will be centrally managed.)*

---
## Workflow Overview
This project follows a structured workflow for training and evaluating a sentiment analysis model, implementing **continuous integration, delivery, and deployment** practices.

Additionally, it includes automated testing and code quality checks through **continuous testing** to ensure robustness and maintainability.

### Continuous Integration

`integration.yml` automatically runs integration tests and code quality checks on every pull request to the `main` branch. The workflow performs the following steps:

* Installs all project dependencies in a clean environment
* Runs static code analysis using pylint and flake8 to enforce code quality and style
* Scans the codebase for security vulnerabilities with bandit
* Uses DVC to pull required data files from remote storage
* Rebuilds the text preprocessor to ensure feature extraction is reproducible
* Executes the full DVC pipeline (`prepare`, `train`, `evaluate`) to verify that all stages run successfully with the latest code and data

This automated process helps ensure that code changes are robust, secure, and do not break the end-to-end machine learning workflow.
### Continuous Delivery
`delivery.yml` automates the delivery of new model versions as pre-releases. The workflow is triggered automatically on every push to the main branch. The workflow performs the following steps:

* Installs all dependencies and pulls the latest data and artifacts using DVC
* Runs the full DVC pipeline to retrain the model and generate updated evaluation metrics
* Uploads the trained model and evaluation metrics as GitHub Actions artifacts for traceability
* Automatically bumps the pre-release version tag using semantic versioning and creates a new GitHub pre-release with the model and metrics attached.
* This workflow ensures that every update to the main branch results in a reproducible, versioned pre-release, making it easy to track and test new model iterations before a stable release

### Continuous Deployment
`deployment.yml` automates the deployment of the sentiment analysis model. The workflow is triggered manually via the GitHub Actions UI and allows you to specify the release level (`patch`, `minor`, or `major`) for versioning. The workflow performs the following steps:

* Installs all project dependencies and pulls the latest data and artifacts using DVC
* Runs the full DVC pipeline to retrain the model and generate updated evaluation metrics
* Uploads the trained model and evaluation metrics as GitHub Actions artifacts for record keeping
* Bumps the stable version tag according to the selected release level and creates a new GitHub release with the model and metrics attached.
* Handles Git operations and SSH setup to automate tagging and commits, ensuring a smooth release process
* Automatically starts the next development cycle by incrementing to the next pre-release version

### Continuous Testing
`testing.yml` ensures code quality and reliability by running automated tests and static analysis on every push and pull request to the main branch. The workflow performs the following steps:

* Installs all dependencies and sets up the Python environment
* Runs pylint for code quality checks and generates a badge based on the score
* Performs security analysis with bandit and outputs results
* Pulls the latest data and artifacts using DVC
* Executes the full DVC pipeline to verify that all stages run successfully
* Runs all unit and integration tests with pytest, collecting code coverage information
* Automatically updates the README with the latest test coverage, adequacy metrics, and security scan results
* Commits and pushes updated badges and metrics to the repository for visibility
* This workflow helps maintain high code standards and provides immediate feedback on the impact of code changes
## Model integration
The trained model can now be integrated and used for predictions

### 1. From a GitHub Release
Each stable version tag `vX.Y.Z` has its model attached as a release asset:
```
https://github.com/remla2025-team9/model-training/releases/download/vX.Y.Z/model.joblib
```
In your code or service, set the URL and fetch at startup:
```python
import os
import pathlib
import requests
import joblib

MODEL_URL = os.getenv(
    "MODEL_URL",
    "https://github.com/remla2025-team9/model-training/releases/download/v1.2.3/"
    "sentiment_pipeline-v1.2.3.joblib"
)
```
Download and cache the model:
```python
cache_dir = pathlib.Path("/cache/models")
cache_dir.mkdir(parents=True, exist_ok=True)
local_path = cache_dir / pathlib.Path(MODEL_URL).name

if not local_path.exists():
    resp = requests.get(MODEL_URL)
    resp.raise_for_status()
    local_path.write_bytes(resp.content)
```
Load the model:
```python
model = joblib.load(local_path)
```
### 2. From a local file
It is also possible to load the model from a local file, either by cloning the repo and following the steps above to train and generate the model, or by downloading the model file directly from the GitHub release page.

## How to Run Tests with Coverage Reporting

You can run the test suite with coverage measurement using the following command:

```bash
pytest --cov=src tests/
```
## Code Quality Checks

We use three tools to ensure code quality: Pylint, Flake8, and Bandit.

### Pylint

To run pylint (checking  naming, code smells, and structure, includes a custom plugin to detect hardcoded seeds)

```bash
pip install pylint
pylint --rcfile=linting/.pylintrc src/
```


### Flake8

To run flake8 (PEP8 formatting)
```bash
pip install flake8
flake8 --config=linting/.flake8 src/
```


### Bandit

To run bandit

```bash
pip install bandit
bandit -r src/ -c linting/bandit.yaml
```


## Coverage Report

<!-- coverage-start -->

| File | Stmts | Miss | Cover |
|------|-------|------|--------|
| `src/__init__.py` | 1 | 0 | 100% |
| `src/config.py` | 13 | 0 | 100% |
| `src/dataset.py` | 60 | 4 | 93% |
| `src/features.py` | 22 | 0 | 100% |
| `src/modeling/__init__.py` | 0 | 0 | 100% |
| `src/modeling/predict.py` | 47 | 2 | 96% |
| `src/modeling/train.py` | 40 | 1 | 98% |
| `src/plots.py` | 0 | 0 | 100% |
| **Total** | 183 | 7 | 96% |

<!-- coverage-end -->

Generated using `pytest` and `pytest-cov`:

| Module Path                  | Stmts | Miss | Cover |
|-----------------------------|-------|------|-------|
| `src/__init__.py`           | 1     | 0    | 100%  |
| `src/config.py`             | 13    | 0    | 100%  |
| `src/dataset.py`            | 60    | 4    | 93%   |
| `src/features.py`           | 22    | 22   | 0%    |
| `src/modeling/__init__.py`  | 0     | 0    | 100%  |
| `src/modeling/predict.py`   | 47    | 2    | 96%   |
| `src/modeling/train.py`     | 40    | 1    | 98%   |
| `src/plots.py`              | 0     | 0    | 100%  |
| **Total**                   | **161** | **7**  | **96%**  |

### Test Summary

- **All 28 test cases passed**
- **Overall coverage: 96%**

## 🧪 Test Adequacy Metrics
<!-- adequacy-start -->
| Metric               | Value   |
|----------------------|---------|
| Accuracy             | 0.7556  |
| Precision (weighted) | 0.7563  |
| Recall (weighted)    | 0.7556  |
| F1 Score (weighted)  | 0.7536  |

<details>
<summary>Confusion Matrix</summary>
[[55, 27], [17, 81]]
</details>
<!-- adequacy-end -->

<!-- BANDIT_START -->
## Bandit Security Analysis

| File | LOC | High | Medium | Low |
|------|-----|------|--------|-----|
| `src/__init__.py` | 2 | 0 | 0 | 0 |
| `src/config.py` | 14 | 0 | 0 | 0 |
| `src/dataset.py` | 115 | 0 | 0 | 0 |
| `src/features.py` | 27 | 0 | 0 | 0 |
| `src/modeling/__init__.py` | 0 | 0 | 0 | 0 |
| `src/modeling/predict.py` | 83 | 0 | 0 | 0 |
| `src/modeling/train.py` | 60 | 0 | 0 | 0 |
| `src/plots.py` | 0 | 0 | 0 | 0 |
<!-- BANDIT_END -->

## Linting Scores
![Pylint Score](linting/pylint_score.svg)